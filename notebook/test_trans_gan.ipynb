{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ae8TloR3jTve"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import numpy as np \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28879,
     "status": "ok",
     "timestamp": 1588570293147,
     "user": {
      "displayName": "Amartya Hatua",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYsXoGdNKr9c4jiOrFY0eOJomjCh-C60nCIO_Ekw=s64",
      "userId": "01432058693271343750"
     },
     "user_tz": 300
    },
    "id": "g1XBDLkNELOW",
    "outputId": "b553e5bc-1025-4828-c72d-67dc1e199c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0yIXTrQFYKP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXMDL7arlNZ9"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def lengthWordToIndex(self):\n",
    "        return len(self.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGfrK69ZlRoM"
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ijn9LRjClUer"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('/content/drive/My Drive/translate/question_answer.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Emcr_586lXay"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27102,
     "status": "ok",
     "timestamp": 1588570624973,
     "user": {
      "displayName": "Amartya Hatua",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYsXoGdNKr9c4jiOrFY0eOJomjCh-C60nCIO_Ekw=s64",
      "userId": "01432058693271343750"
     },
     "user_tz": 300
    },
    "id": "yTbPJ-JclaKm",
    "outputId": "1c08e8b3-db5c-4b17-e8a0-e542f388e368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 699281 sentence pairs\n",
      "Trimmed to 12879 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "gar 80\n",
      "eng 81\n",
      "['how are people there ?', 'they re friendly and hospitable .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'gar', True)\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVTZGtAjleww"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93-pgjqElzDZ"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnuLZm1al8nM"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10225,
     "status": "error",
     "timestamp": 1588570488326,
     "user": {
      "displayName": "Amartya Hatua",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYsXoGdNKr9c4jiOrFY0eOJomjCh-C60nCIO_Ekw=s64",
      "userId": "01432058693271343750"
     },
     "user_tz": 300
    },
    "id": "1RAvdiFrmRzC",
    "outputId": "f137f50a-822c-47cd-f852-09f3b6b5f761"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-41c2fd1fca29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/translate/encoder_qna2.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([58, 256]) from checkpoint, the shape in current model is torch.Size([80, 256])."
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "encoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/encoder_qna2.dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11932,
     "status": "ok",
     "timestamp": 1588371993731,
     "user": {
      "displayName": "Amartya Hatua",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYsXoGdNKr9c4jiOrFY0eOJomjCh-C60nCIO_Ekw=s64",
      "userId": "01432058693271343750"
     },
     "user_tz": 300
    },
    "id": "JkKfekl9mZr6",
    "outputId": "73bca51b-4a62-41da-b4d5-c3dc6fc1b41e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "attn_decoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/decoder_qan2.dict'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX7qzeOAmnSZ"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, block_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(block_dim, block_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(block_dim, block_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) + x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, block_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            *[Block(block_dim) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pVGmTBZn9OB"
   },
   "outputs": [],
   "source": [
    "def testGan(encoder, decoder, sentence, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        # print(\"input tensor\",input_tensor.shape)\n",
    "        # print(input_tensor)\n",
    "        # input_length = input_tensor.size()[0]\n",
    "        # print(\"input tensor\",input_length)\n",
    "        # encoder_hidden = encoder.initHidden()\n",
    "        # print(\"encoder_hidden\",encoder_hidden.shape)\n",
    "        # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        # print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "\n",
    "        # for ei in range(input_length):\n",
    "        #     encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        #     #print(encoder_output[0, 0].shape)\n",
    "        #     encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        #     print(encoder_outputs)\n",
    "\n",
    "          \n",
    "\n",
    "        latent_dim = 256\n",
    "        n_layers = 20\n",
    "        block_dim = 256\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        input_length = sentence.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        #encoder_outputs[0: input_length,:] = sentence\n",
    "        generator = Generator(n_layers, block_dim)\n",
    "        for ei in range(input_length):\n",
    "            noise = torch.from_numpy(np.random.randint(-1, 1, (10, 256))).float()\n",
    "            z = generator(noise)\n",
    "            #encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            #print(encoder_output[0, 0].shape)\n",
    "            encoder_outputs[ei] += z[0, 0]\n",
    "            #print(encoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        # print(\"decoder_input\",decoder_input.shape)\n",
    "        # print(\"decoder_hidden\",decoder_hidden.shape)\n",
    "        # print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if ((topi.item() == EOS_token) or (topi.item() == '.')):\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbybWM_anoJH"
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "def evaluateGAN(encoder, decoder, n=200):\n",
    "    latent_dim = 256\n",
    "    n_layers = 20\n",
    "    block_dim = 256\n",
    "\n",
    "    for i in range(n):\n",
    "        generator = Generator(n_layers, block_dim)\n",
    "        generator.eval()\n",
    "        generator.load_state_dict(torch.load('/content/drive/My Drive/translate/generator_qna2.th', map_location='cpu'))\n",
    "        input_tensor = [[85],[86],[109],[1362],[2],[6],[1]]\n",
    "        input_tensor = np.asarray(input_tensor)\n",
    "        noise = torch.from_numpy(np.random.randint(1, 4434, (10, latent_dim)))\n",
    "        #noise = torch.from_numpy(input_tensor).float()\n",
    "        #z = generator(noise)\n",
    "        #print(\"Z shape=\",z.shape)\n",
    "        #print(\"z=\", z)\n",
    "        pair = random.choice(pairs)\n",
    "        #print('>', pair[0])\n",
    "        #print('=', pair[1])\n",
    "        output_words, attentions = testGan(encoder, decoder, noise)\n",
    "        #output_words, attentions = testGan(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19079,
     "status": "ok",
     "timestamp": 1588373984615,
     "user": {
      "displayName": "Amartya Hatua",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYsXoGdNKr9c4jiOrFY0eOJomjCh-C60nCIO_Ekw=s64",
      "userId": "01432058693271343750"
     },
     "user_tz": 300
    },
    "id": "1bqo9iSbn8V9",
    "outputId": "888d9cb6-42ec-4aaa-dd8f-b5e486110690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< i . <EOS>\n",
      "< i m about to do . <EOS>\n",
      "< you re making me me . . . <EOS>\n",
      "< you re making up to the . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you re kidding to be in . <EOS>\n",
      "< we . . . <EOS>\n",
      "< you re up to . <EOS>\n",
      "< you . <EOS>\n",
      "< i m thinking . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< you re making a student . . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< you re making the whole . . . <EOS>\n",
      "< you re up to be here . <EOS>\n",
      "< you re making my of me . <EOS>\n",
      "< i . <EOS>\n",
      "< you re a my . <EOS>\n",
      "< you are free . <EOS>\n",
      "< you re making a . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i m thinking . . . <EOS>\n",
      "< i m good at the situation . <EOS>\n",
      "< you re up a . <EOS>\n",
      "< i m about to do it . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< i . . . . . . . . .\n",
      "< you are free . <EOS>\n",
      "< i m getting it . <EOS>\n",
      "< you re a student . <EOS>\n",
      "< you re making a . . . . <EOS>\n",
      "< you . <EOS>\n",
      "< you re making the same age . <EOS>\n",
      "< you re making a . <EOS>\n",
      "< . <EOS>\n",
      "< you re in the same age . <EOS>\n",
      "< you re under at the . . <EOS>\n",
      "< you re up to be good . <EOS>\n",
      "< i m coming at the news . <EOS>\n",
      "< i m in the point . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you re making my way . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< you re kidding . . <EOS>\n",
      "< we . . . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< we . . . . . . . . .\n",
      "< you re kidding . <EOS>\n",
      "< i . <EOS>\n",
      "< you are free a . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re a high . . . . . .\n",
      "< you re in a arrest . <EOS>\n",
      "< you re making a arrest . <EOS>\n",
      "< i m in a hurry to you . <EOS>\n",
      "< you re making up to me . . . <EOS>\n",
      "< i m about to be here . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re in a . <EOS>\n",
      "< we re losing . . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< you re a high . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you ? . <EOS>\n",
      "< you re up to . . <EOS>\n",
      "< you . . . . <EOS>\n",
      "< we . . . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i aren t . . . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< you re making a . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i re up to . . . <EOS>\n",
      "< i . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i m up to . . . . <EOS>\n",
      "< you . . . . . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< you re making a . . . . . <EOS>\n",
      "< you . <EOS>\n",
      "< i re up . . . . . . .\n",
      "< i m in the same age . <EOS>\n",
      "< you re making a arrest . . <EOS>\n",
      "< you re making my way . <EOS>\n",
      "< i m in the point . . <EOS>\n",
      "< i m sorry to think . . . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< you re making a arrest . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i re going to be . . <EOS>\n",
      "< i re up . . . . . . .\n",
      "< we re up . <EOS>\n",
      "< i m sorry . . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< . <EOS>\n",
      "< you . . . . . . . <EOS>\n",
      "< we re up . . . . . . .\n",
      "< you re up to the me . <EOS>\n",
      "< i m about to . . . <EOS>\n",
      "< you re playing right . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< i m sorry . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< you re in the same age . <EOS>\n",
      "< i re up . <EOS>\n",
      "< we . . <EOS>\n",
      "< i m having the same . <EOS>\n",
      "< we . . <EOS>\n",
      "< i m getting old and . . . <EOS>\n",
      "< i m in the same age . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i re going to the . . . <EOS>\n",
      "< you re a good and . . . <EOS>\n",
      "< you re up to me . <EOS>\n",
      "< you are free a . <EOS>\n",
      "< you re kidding to the . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you re playing a . . . <EOS>\n",
      "< i re up to the . . <EOS>\n",
      "< you re making me . <EOS>\n",
      "< i re up to . . . <EOS>\n",
      "< you re a about the . . <EOS>\n",
      "< you re making a walk . . . <EOS>\n",
      "< you re up . <EOS>\n",
      "< you re up to be . . <EOS>\n",
      "< you re making a . . . . . .\n",
      "< you re kidding to the . . <EOS>\n",
      "< i re getting . . <EOS>\n",
      "< we . <EOS>\n",
      "< you re up to be here . . . <EOS>\n",
      "< you are . <EOS>\n",
      "< you re up to be . . <EOS>\n",
      "< you re kidding . . . . . <EOS>\n",
      "< i m sorry about the . . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you ? <EOS>\n",
      "< you are free . <EOS>\n",
      "< you re a good we . . <EOS>\n",
      "< you are free a . <EOS>\n",
      "< i m sorry . . . . . . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< we . . . . . . . . .\n",
      "< you re making a . . . . . .\n",
      "< you re making me . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< i re doing . . . . . . .\n",
      "< i m sorry . <EOS>\n",
      "< we re up to . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i . . . . . . <EOS>\n",
      "< you re joking my own . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you re making a friend . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< i m sorry for you . . . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< . . . . <EOS>\n",
      "< i m getting it . <EOS>\n",
      "< we re up . <EOS>\n",
      "< you re under at my way . <EOS>\n",
      "< i m up to the . . <EOS>\n",
      "< you re making a arrest . <EOS>\n",
      "< i m in the point . . . <EOS>\n",
      "< you re making the whole . . . . .\n",
      "< i . . <EOS>\n",
      "< we re up . . . . . . .\n",
      "< you . <EOS>\n",
      "< we . . <EOS>\n",
      "< you re kidding . . . . . . .\n",
      "< you re up to me . <EOS>\n",
      "< i m sorry . . . <EOS>\n",
      "< you re a to my own . <EOS>\n",
      "< you re up to the . . . . .\n",
      "< you re making a boy . <EOS>\n",
      "< i m sorry . <EOS>\n",
      "< i m sorry to the . . . . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< you are free . <EOS>\n",
      "< you re making a walk . <EOS>\n",
      "< i . . . . . . . . .\n",
      "< you re kidding . <EOS>\n",
      "< you is good at the moment . . <EOS>\n",
      "< you re a to be . <EOS>\n",
      "< i . . . . . <EOS>\n",
      "< you are free . <EOS>\n",
      "< you re kidding . <EOS>\n",
      "< i m up to . <EOS>\n",
      "< we re . . . . . . . .\n",
      "< i re kidding . <EOS>\n",
      "< you re making a my . <EOS>\n",
      "< you re making my way . <EOS>\n",
      "< you re making a friend . <EOS>\n"
     ]
    }
   ],
   "source": [
    "evaluateGAN(encoder1,attn_decoder1)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_trans_gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
